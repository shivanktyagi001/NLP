{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO9DkWmYB3iaQ/EKr7etM0y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shivanktyagi001/NLP/blob/main/NLP_03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWejU9ZNLO2y",
        "outputId": "c1b2e5dd-a828-4a9b-8dad-a1a867f3fd53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i am learning python and python!!', 'we are happy today!', 'i am ,sad today']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import string\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"punkt_tab\")\n",
        "\n",
        "punctuation = string.punctuation\n",
        "from nltk.tokenize import  word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wnet = WordNetLemmatizer()\n",
        "sentences = [\n",
        "    \"I am learning python and python!!\",\n",
        "    \"We are happy today!\",\n",
        "    \"I am ,sad today\",\n",
        "]\n",
        "\n",
        "def preprocess_text(docs):\n",
        "  lower_docs = []\n",
        "  for word in docs:\n",
        "    lower_docs.append(word.lower())\n",
        "  return lower_docs\n",
        "print(preprocess_text(sentences))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def preprocess_text(docs):\n",
        "  lower_docs = []\n",
        "  for word in docs:\n",
        "    lower_docs.append(word.lower())\n",
        "  token_list = []\n",
        "  for word in lower_docs:\n",
        "    token_list.append(word_tokenize(word))\n",
        "  ###########################################\n",
        "  english_stopwords= stopwords.words(\"english\")\n",
        "  filtered_token = []\n",
        "  for word in token_list:\n",
        "    new_list = []\n",
        "    for i in word:\n",
        "      if i not in english_stopwords:\n",
        "        new_list.append(i)\n",
        "    filtered_token.append(new_list)\n",
        "  #########################################\n",
        "  clean_token = []\n",
        "  for word in filtered_token:\n",
        "    new_list = []\n",
        "    for i in word:\n",
        "      if i not in punctuation:\n",
        "        new_list.append(i)\n",
        "    clean_token.append(new_list)\n",
        "\n",
        "    ###################################\n",
        "  final_token = []\n",
        "  for word in clean_token:\n",
        "    new_list = []\n",
        "    for i in word:\n",
        "      new_list.append(wnet.lemmatize(i,\"v\"))\n",
        "    final_token.append(new_list)\n",
        "  ans_doc = []\n",
        "  for word in final_token:\n",
        "    ans_doc.append(\" \".join(word))\n",
        "  return ans_doc\n",
        "################################################################\n",
        "\n",
        "print(preprocess_text(sentences))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwWpFmXOMpi5",
        "outputId": "1603f77a-0be0-4432-b3a2-e7d6739535e1"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['learn python python', 'happy today', 'sad today']\n"
          ]
        }
      ]
    }
  ]
}